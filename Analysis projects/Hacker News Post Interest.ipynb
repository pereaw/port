{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hacker News Post Interest Analysis\n",
    "\n",
    "Hacker News is a site started by the startup incubator Y Combinator, where user-submitted stories (known as \"posts\") receive votes and comments, similar to reddit. Hacker News is extremely popular in technology and startup circles, and posts that make it to the top of the Hacker News listings can get hundreds of thousands of visitors as a result.\n",
    "\n",
    "We're specifically interested in posts with titles that begin with either Ask HN or Show HN. Users submit Ask HN posts to ask the Hacker News community a specific question. Below are a few examples:\n",
    "\n",
    "* Ask HN: How to improve my personal website?\n",
    "* Ask HN: Am I the only one outraged by Twitter shutting down share counts?\n",
    "* Ask HN: Aby recent changes to CSS that broke mobile?\n",
    "\n",
    "Likewise, users submit Show HN posts to show the Hacker News community a project, product, or just something interesting. Below are a few examples:\n",
    "\n",
    "* Show HN: Wio Link  ESP8266 Based Web of Things Hardware Development Platform'\n",
    "* Show HN: Something pointless I made\n",
    "* Show HN: Shanhu.io, a programming playground powered by e8vm\n",
    "\n",
    "We'll compare these two types of posts to determine the following:\n",
    "\n",
    "1. Do Ask HN or Show HN receive more comments on average?\n",
    "2. Do posts created at a certain time receive more comments on average?\n",
    "\n",
    "Let's start by importing the libraries we need and reading the dataset into a list of lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['id', 'title', 'url', 'num_points', 'num_comments', 'author', 'created_at'], ['12579008', 'You have two days to comment if you want stem cells to be classified as your own', 'http://www.regulations.gov/document?D=FDA-2015-D-3719-0018', '1', '0', 'altstar', '9/26/2016 3:26'], ['12579005', 'SQLAR  the SQLite Archiver', 'https://www.sqlite.org/sqlar/doc/trunk/README.md', '1', '0', 'blacksqr', '9/26/2016 3:24'], ['12578997', 'What if we just printed a flatscreen television on the side of our boxes?', 'https://medium.com/vanmoof/our-secrets-out-f21c1f03fdc8#.ietxmez43', '1', '0', 'pavel_lishin', '9/26/2016 3:19'], ['12578989', 'algorithmic music', 'http://cacm.acm.org/magazines/2011/7/109891-algorithmic-composition/fulltext', '1', '0', 'poindontcare', '9/26/2016 3:16']]\n"
     ]
    }
   ],
   "source": [
    "from csv import reader\n",
    "\n",
    "opened_file = open('D:/Library/datasci/datasets/HN_posts_year_to_Sep_26_2016.csv', encoding = \"utf-8\")\n",
    "read_file = reader(opened_file)\n",
    "hn = list(read_file)\n",
    "\n",
    "print(hn[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id', 'title', 'url', 'num_points', 'num_comments', 'author', 'created_at']\n",
      "[['12579008', 'You have two days to comment if you want stem cells to be classified as your own', 'http://www.regulations.gov/document?D=FDA-2015-D-3719-0018', '1', '0', 'altstar', '9/26/2016 3:26'], ['12579005', 'SQLAR  the SQLite Archiver', 'https://www.sqlite.org/sqlar/doc/trunk/README.md', '1', '0', 'blacksqr', '9/26/2016 3:24'], ['12578997', 'What if we just printed a flatscreen television on the side of our boxes?', 'https://medium.com/vanmoof/our-secrets-out-f21c1f03fdc8#.ietxmez43', '1', '0', 'pavel_lishin', '9/26/2016 3:19'], ['12578989', 'algorithmic music', 'http://cacm.acm.org/magazines/2011/7/109891-algorithmic-composition/fulltext', '1', '0', 'poindontcare', '9/26/2016 3:16'], ['12578979', 'How the Data Vault Enables the Next-Gen Data Warehouse and Data Lake', 'https://www.talend.com/blog/2016/05/12/talend-and-Â\\x93the-data-vaultÂ\\x94', '1', '0', 'markgainor1', '9/26/2016 3:14']]\n"
     ]
    }
   ],
   "source": [
    "#extract first row of data and assign it to the headers variable.\n",
    "headers = hn[0]\n",
    "hn = hn[1:]\n",
    "print(headers)\n",
    "print(hn[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting Posts of Interest\n",
    "\n",
    "We're only interested in the posts starting with Ask HN and Show HN. Lets pull these from the dataset and put them into their own lists.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9139 10158 282961\n"
     ]
    }
   ],
   "source": [
    "#initialize empty lists\n",
    "ask_posts = []\n",
    "show_posts = []\n",
    "other_posts = []\n",
    "\n",
    "\n",
    "#loop over the rows in the dataset.\n",
    "for row in hn:\n",
    "    #assign the title of the post to a variable\n",
    "    title = row[1]\n",
    "    #append entire row off of starting string in title\n",
    "    if title.lower().startswith(\"ask hn\"):\n",
    "        ask_posts.append(row)\n",
    "    if title.lower().startswith(\"show hn\"):\n",
    "        show_posts.append(row)\n",
    "    else:\n",
    "        other_posts.append(row)\n",
    "\n",
    "print(len(ask_posts), len(show_posts), len(other_posts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Average Number of Comments by Posting Type\n",
    "\n",
    "Now we check if the show or ask posts recieve more comments on average.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average ask comments: 10.39 Average show comments: 4.89\n"
     ]
    }
   ],
   "source": [
    "#initialize empty count\n",
    "total_ask_comments = 0\n",
    "total_show_comments = 0\n",
    "\n",
    "#start adding\n",
    "for row in ask_posts:\n",
    "    num = int(row[4])\n",
    "    total_ask_comments = total_ask_comments + num\n",
    "\n",
    "for row in show_posts:\n",
    "    num = int(row[4])\n",
    "    total_show_comments = total_show_comments + num\n",
    "\n",
    "#calculate the average\n",
    "avg_ask_comments = total_ask_comments/len(ask_posts)  \n",
    "avg_show_comments = total_show_comments/len(show_posts)\n",
    "\n",
    "#print averages\n",
    "print((\"Average ask comments: {} Average show comments: {}\".format(round(avg_ask_comments,2),round(avg_show_comments,2))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see on average that Ask HN posts get about twice as many comments than the Show HN posts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Average Number of Comments by Hour\n",
    "\n",
    "As Ask HN posts result in significantly more comments we will focus only on the Ask HN dataset. Next, we are investigating whether posts created at a certain time are more likely to attract comments. To do so we will perform the following steps:\n",
    "\n",
    "1. Calculate the number of ask posts created in each hour of the day, along with the number of comments recieved.\n",
    "2. Calculate the average number of comments ask posts receive by hour created.\n",
    "\n",
    "We will work on the first step using the datetime module to work with the data in the created_at column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First we create a list of the relevant data, then we will populate two dictionaries to hold the count and comment data by hour.\n",
    "\n",
    "#import datetime\n",
    "import datetime as dt\n",
    "\n",
    "result_list = []\n",
    "\n",
    "#pull the created_at column(index 7) and number column (index 5)\n",
    "for row in ask_posts:\n",
    "    temp = [row[6], int(row[4])]\n",
    "    result_list.append(temp)\n",
    "\n",
    "#empty dictionaries to populate\n",
    "counts_by_hour = {}\n",
    "comments_by_hour = {}\n",
    "\n",
    "\n",
    "for row in result_list:\n",
    "    #store time data and comments\n",
    "    hour = row[0]\n",
    "    comments = row[1]\n",
    "    dt_time = dt.datetime.strptime(hour, \"%m/%d/%Y %H:%M\")    #convert the time to a datetime object\n",
    "    dt_hour = dt.datetime.strftime(dt_time, \"%H\")     #output just the hour\n",
    "    \n",
    "    #assign to dictionaries and count\n",
    "    if dt_hour not in counts_by_hour:\n",
    "        counts_by_hour[dt_hour] = 1\n",
    "        comments_by_hour[dt_hour] = comments\n",
    "    else:\n",
    "        counts_by_hour[dt_hour] +=1\n",
    "        comments_by_hour[dt_hour] += comments\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['02', 11.137546468401487], ['01', 7.407801418439717], ['22', 8.804177545691905], ['21', 8.687258687258687], ['19', 7.163043478260869], ['17', 9.449744463373083], ['15', 28.676470588235293], ['14', 9.692007797270955], ['13', 16.31756756756757], ['11', 8.96474358974359], ['10', 10.684397163120567], ['09', 6.653153153153153], ['07', 7.013274336283186], ['03', 7.948339483394834], ['23', 6.696793002915452], ['20', 8.749019607843136], ['16', 7.713298791018998], ['08', 9.190661478599221], ['00', 7.5647840531561465], ['18', 7.94299674267101], ['12', 12.380116959064328], ['04', 9.7119341563786], ['06', 6.782051282051282], ['05', 8.794258373205741]] 2\n"
     ]
    }
   ],
   "source": [
    "#calculate the averages\n",
    "avg_by_hour = []\n",
    "\n",
    "for hour in comments_by_hour:\n",
    "    avg_by_hour.append([hour, comments_by_hour[hour]/counts_by_hour[hour]])\n",
    "    \n",
    "print(avg_by_hour)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now run a bit of code to make things more readable and print out the top 5 hours by post count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15:00: 28.68 average comments per post\n",
      "13:00: 16.32 average comments per post\n",
      "12:00: 12.38 average comments per post\n",
      "02:00: 11.14 average comments per post\n",
      "10:00: 10.68 average comments per post\n"
     ]
    }
   ],
   "source": [
    "#Create a list with swapped columns equivalent to avg_by_hour\n",
    "swap_avg_by_hour = []\n",
    "\n",
    "for row in avg_by_hour:\n",
    "    swap_avg_by_hour.append([row[1], row[0]])\n",
    "\n",
    "sorted_swap = sorted(swap_avg_by_hour, reverse = True)\n",
    "\n",
    "for row in sorted_swap[:5]:\n",
    "    count = row[0]\n",
    "    dtime = dt.datetime.strptime(row[1], \"%H\") #convert to datetime object\n",
    "    hour = dt.datetime.strftime(dtime, \"%H:00\") #pull the hour and format\n",
    "    print(\"{}: {:.2f} average comments per post\".format(hour, count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the hour with the most average comments per post is 15:00 Eastern Time. If a poster is interested in receiving comments and potentially traction on the site they should be making a post in the early afternoon adjusted for their timezone.\n",
    "\n",
    "\n",
    "#### Gauging Reader Value\n",
    "\n",
    "The dataset additionally contains information on the points a post receives. A post receiving more points can indicate number of viewers and some measure of interaction or value. A similar analysis can be done to check the average number of points based off of post type, and time. This can then be compared to what we have produced already summarizing the average comments per post by time. To do so we will perform the following steps:\n",
    "\n",
    "1. Determine if show or ask posts receive more points on average.\n",
    "2. Determine if posts created at a certain time are more likely to receive more points.\n",
    "3. Compare your results to the average number of comments and points other posts receive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average ask points: 11.31 Average show points: 14.84\n"
     ]
    }
   ],
   "source": [
    "#initialize empty count\n",
    "total_ask_points = 0\n",
    "total_show_points = 0\n",
    "\n",
    "#start adding\n",
    "for row in ask_posts:\n",
    "    num = int(row[3])\n",
    "    total_ask_points = total_ask_points + num\n",
    "\n",
    "for row in show_posts:\n",
    "    num = int(row[3])\n",
    "    total_show_points = total_show_points + num\n",
    "\n",
    "#calculate the average\n",
    "avg_ask_points = total_ask_points/len(ask_posts)  \n",
    "avg_show_points = total_show_points/len(show_posts)\n",
    "\n",
    "#print averages\n",
    "print((\"Average ask points: {} Average show points: {}\".format(round(avg_ask_points,2),round(avg_show_points,2))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average number of points that a post receives is higher for the Show HN posts. This is in contrast with the results for number of comments but it is not unreasonable to assume that asking will result in more people posting comments to essentially answer the poster. Show HN posts might be more interesting or valuable to a larger base of readers. \n",
    "\n",
    "However, as we have done an analysis already on the Ask HN posts, I will continue working on that dataset. A more comprehensive analysis may be done on both datasets if desired.\n",
    "\n",
    "Lets execute some very similar code and see what we get.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['02', 10.944237918215613], ['01', 9.439716312056738], ['22', 9.402088772845953], ['21', 9.733590733590734], ['19', 8.66304347826087], ['17', 12.189097103918229], ['15', 21.637770897832816], ['14', 10.50682261208577], ['13', 17.93243243243243], ['11', 9.153846153846153], ['10', 13.436170212765957], ['09', 7.941441441441442], ['07', 9.026548672566372], ['03', 9.3690036900369], ['23', 7.626822157434402], ['20', 8.805882352941177], ['16', 10.310880829015543], ['08', 10.67704280155642], ['00', 9.418604651162791], ['18', 11.156351791530945], ['12', 13.576023391812866], ['04', 10.905349794238683], ['06', 8.675213675213675], ['05', 9.789473684210526]]\n"
     ]
    }
   ],
   "source": [
    "result_list2 = []\n",
    "\n",
    "#pull the created_at column(index 7) and point column (index 4)\n",
    "for row in ask_posts:\n",
    "    temp = [row[6], int(row[3])]\n",
    "    result_list2.append(temp)\n",
    "\n",
    "#empty dictionaries to populate\n",
    "counts_by_hour2 = {}\n",
    "points_by_hour = {}\n",
    "\n",
    "\n",
    "for row in result_list2:\n",
    "    #store time data and comments\n",
    "    hour = row[0]\n",
    "    points = row[1]\n",
    "    dt_time = dt.datetime.strptime(hour, \"%m/%d/%Y %H:%M\")    #convert the time to a datetime object\n",
    "    dt_hour = dt.datetime.strftime(dt_time, \"%H\")     #output just the hour\n",
    "    \n",
    "    #assign to dictionaries and count\n",
    "    if dt_hour not in counts_by_hour2:\n",
    "        counts_by_hour2[dt_hour] = 1\n",
    "        points_by_hour[dt_hour] = points\n",
    "    else:\n",
    "        counts_by_hour2[dt_hour] +=1\n",
    "        points_by_hour[dt_hour] += points\n",
    "        \n",
    "#calculate the averages\n",
    "avg_by_hour2 = []\n",
    "\n",
    "for hour in points_by_hour:\n",
    "    avg_by_hour2.append([hour, points_by_hour[hour]/counts_by_hour[hour]])\n",
    "    \n",
    "print(avg_by_hour2)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15:00: 21.64 average points per post\n",
      "13:00: 17.93 average points per post\n",
      "12:00: 13.58 average points per post\n",
      "10:00: 13.44 average points per post\n",
      "17:00: 12.19 average points per post\n"
     ]
    }
   ],
   "source": [
    "#Create a list with swapped columns equivalent to avg_by_hour\n",
    "swap_avg_by_hour2 = []\n",
    "\n",
    "for row in avg_by_hour2:\n",
    "    swap_avg_by_hour2.append([row[1], row[0]])\n",
    "\n",
    "sorted_swap2 = sorted(swap_avg_by_hour2, reverse = True)\n",
    "\n",
    "for row in sorted_swap2[:5]:\n",
    "    count = row[0]\n",
    "    dtime = dt.datetime.strptime(row[1], \"%H\") #convert to datetime object\n",
    "    hour = dt.datetime.strftime(dtime, \"%H:00\") #pull the hour and format\n",
    "    print(\"{}: {:.2f} average points per post\".format(hour, count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusions\n",
    "\n",
    "We find that the average number of points per post is highest at 15:00 Eastern Time once again. This time likely corresponds to the highest traffic that the website recieves. If we are interested in making a post with high reader engagement that is certainly the time to make it if we are making an Ask HN post.\n",
    "\n",
    "Further analysis could be done to check if there is any difference in results based on the Show HN posts or the Other category of posts. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new",
   "language": "python",
   "name": "new"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
